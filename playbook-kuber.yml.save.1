- name: Configure HTCondor Cluster with Kubernetes (dynamic version)
  hosts: all
  gather_facts: yes
  become: yes
  vars:
    condor_cluster_password: "qwerty"
    central_manager_ip: "192.168.56.101"
    condor_config_dir: "/etc/condor"
    condor_logs_dir: "/var/log/condor"
    condor_image: "evgenii457/htcondor2"
    kube_bin_dir: "/usr/local/bin"
    venv_path: "/opt/htcondor_venv"
    ansible_python_interpreter: "{{ venv_path }}/bin/python"

  pre_tasks:
    - name: Получение последней стабильной версии Kubernetes
      shell: |
        curl -L -s https://dl.k8s.io/release/stable.txt
      register: k8s_version
      changed_when: false
      delegate_to: localhost
      run_once: yes

  tasks:
    - name: Проверка наличия curl
      package:
        name: curl
        state: present

    - name: Создание временной директории для загрузки Kubernetes
      file:
        path: "/tmp/kubernetes"
        state: directory
        mode: 0755

    - name: Скачивание kubelet
      get_url:
        url: "https://dl.k8s.io/release/v1.33.2/bin/linux/amd64/kubelet"
        dest: "{{ kube_bin_dir }}/kubelet"
        mode: 0755

    - name: Скачивание kubeadm
      get_url:
        url: "https://dl.k8s.io/release/v1.33.2/bin/linux/amd64/kubeadm"
        dest: "{{ kube_bin_dir }}/kubeadm"
        mode: 0755

    - name: Скачивание kubectl
      get_url:
        url: "https://dl.k8s.io/release/v1.33.2/bin/linux/amd64/kubectl"
        dest: "{{ kube_bin_dir }}/kubectl"
        mode: 0755

    - name: Проверка установленных компонентов
      command: "{{ item.cmd }}"
      loop:
        - { cmd: "{{ kube_bin_dir }}/kubelet  --version" }
        - { cmd: "{{ kube_bin_dir }}/kubeadm version" }
        - { cmd: "{{ kube_bin_dir }}/kubectl version --client" }
      loop_control:
        label: "{{ item.cmd }}"
      register: kubernetes_check
      changed_when: false

    - name: Установка containerd
      package:
        name: containerd
        state: present
      become: yes
      when: "'execute_nodes' in group_names"

    - name: Проверка containerd
      service:
        name: containerd
        state: started
        enabled: yes

    - name: Очистка старого кластера
      command: "{{ kube_bin_dir }}/kubeadm reset --force"
      ignore_errors: yes

    - name: Удаление старых конфигов
      file:
        path: "/etc/kubernetes"
        state: absent

    - name: Удаление старых сертификатов
      file:
        path: "/etc/kubernetes/pki"
        state: absent



    - name: Настройка containerd
      shell: |
        sudo mkdir -p /etc/containerd
        sudo containerd config default > /etc/containerd/config.toml
        sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
        sudo sed -i 's/sandbox_image = "registry.k8s.io\/pause:3.8"/sandbox_image = "registry.aliyuncs.com\/google_containers\/pause:3.10"/' /etc/containerd/config.toml
        sudo systemctl restart containerd
    - name: Настройка kubelet
      shell: |
        echo "KUBELET_KUBEADM_EXTRA_ARGS=--cgroup-driver=systemd" | sudo tee /etc/default/kubelet
        sudo systemctl daemon-reload
        sudo systemctl restart kubelet
    - name: Проверка использования порта 6443
      shell: lsof -i :6443 || echo "Порт 6443 свободен"
      ignore_errors: yes

    - name: Обновление образа pause
      command: crictl pull registry.aliyuncs.com/google_containers/pause:3.10



    - name: Отключение swap
      shell: |
        swapoff -a
        sed -i '/ swap / s/^/#/' /etc/fstab
      register: swap_result
      changed_when: "'swapoff' in swap_result.stdout"

    - name: Инициализация кластера
      command: |
        {{ kube_bin_dir }}/kubeadm init \
          --control-plane-endpoint "{{ central_manager_ip }}" \
          --apiserver-advertise-address="{{ central_manager_ip }}" \
          --apiserver-cert-extra-sans="htcondor-manager.default.svc.cluster.local" \
          --apiserver-cert-extra-sans="{{ central_manager_ip }}" \
          --pod-network-cidr=10.244.0.0/16 \
          --image-repository registry.aliyuncs.com/google_containers \
          --ignore-preflight-errors=all
      register: cluster_init
      when: "'central_manager' in group_names"
      run_once: true
      delegate_to: central_manager

    - name: Получение join команды через kubeadm token
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      delegate_to: central_manager
      run_once: true
      register: join_output
      command: "{{ kube_bin_dir }}/kubeadm token create --print-join-command"
      when: "'central_manager' in group_names"

    - name: Установка join команды
      set_fact:
        join_command: "{{ join_output.stdout }}"
      when: "'central_manager' in group_names"
      run_once: true
      delegate_to: central_manager

    - name: Отладка join_command
      debug:
        msg: "JOIN_COMMAND: {{ join_command }}"
      when: join_command is defined

    - name: Добавление нод в кластер
      command: "{{ join_command }}"
      loop: "{{ groups['execute_nodes'] }}"
      loop_control:
        loop_var: execute_node
      delegate_to: "{{ execute_node }}"
      when:
        - "'execute_nodes' in group_names"
        - join_command is defined
        - join_command != ''


    - name: Настройка KUBECONFIG для root
      shell: |
        mkdir -p /root/.kube
        cp /etc/kubernetes/admin.conf /root/.kube/config
        chown -R root:root /root/.kube
      delegate_to: central_manager
      when: "'central_manager' in group_names"
    - name: Создание директорий
      shell: |
        mkdir -p /home/usermaster/.kube

        sudo cp /etc/kubernetes/admin.conf /home/usermaster/.kube/config
        sudo chown usermaster:usermaster /home/usermaster/.kube/config
        chmod 600 /home/usermaster/.kube/config

        sudo cp /etc/kubernetes/pki/ca.crt /usr/local/share/ca-certificates/k8s-ca.crt
        sudo update-ca-certificates > /dev/null

        echo 'export KUBECONFIG=/home/usermaster/.kube/config' >> /home/usermaster/.bashrc
        sudo -u usermaster bash -c "source /home/usermaster/.bashrc"
        sudo -u usermaster bash -c "export KUBECONFIG=/home/usermaster/.kube/config"
        sudo mkdir -p /var/log/condor
      args:
        executable: /bin/bash
      when: "'central_manager' in group_names"


    - name: Получить текущего пользователя
      command: whoami
      register: current_user
      changed_when: false

    - name: Создать .kube директорию
      file:
        path: "/home/{{ current_user.stdout }}/.kube"
        state: directory
        mode: 0700
        owner: "{{ current_user.stdout }}"
        group: "{{ current_user.stdout }}"
      when: "'execute_nodes' in group_names"


    - name: Обновить адрес API-сервера
      replace:
        path: "/home/{{ current_user.stdout }}/.kube/config"
        regexp: 'server: https://\S+:6443'
        replace: 'server: https://{{ groups["central_manager"][0] }}:6443'
      when: "'execute_nodes' in group_names"


    - name: Создание директории .kube для root
      file:
        path: /root/.kube
        state: directory
        mode: 0700
        owner: root
        group: root
      become: yes
      when: "'execute_nodes' in group_names"

    - name: Перезапуск kubelet после инициализации
      service:
        name: kubelet
        state: restarted
      when: "'central_manager' in group_names"

    - name: Проверка, доступен ли кластер
      command: kubectl cluster-info
      environment:
        KUBECONFIG: "/root/.kube/config"
      register: cluster_info
      retries: 15
      delay: 10
      until: cluster_info.rc == 0
      when: "'central_manager' in group_names"
      ignore_errors: no

    - name: Установка Flannel CNI
      command: |
        kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml  --validate=false
      environment:
        KUBECONFIG: "/root/.kube/config"
      delegate_to: central_manager
      register: flannel_result
      retries: 5
      delay: 10
      until: flannel_result.rc == 0

    - name: Создание ConfigMap для Central Manager
      kubernetes.core.k8s:
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: htcondor-central
            namespace: default
          data:
            condor_config: |
              {{ lookup('template', 'condor_config.central_submit.j2') }}
      when: "'central_manager' in group_names"
      delegate_to: central_manager
      run_once: true

    - name: Создание ConfigMap для Execute Nodes
      kubernetes.core.k8s:
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: htcondor-execute
            namespace: default
          data:
            condor_config: |
              {{ lookup('template', 'condor_config.execute2.j2') }}
      when: "'central_manager' in group_names"
      delegate_to: central_manager
      run_once: true

    - name: Создание Service для Central Manager
      kubernetes.core.k8s:
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: htcondor-manager
            namespace: default
          spec:
            selector:
              app: htcondor
              role: manager
            ports:
              - protocol: TCP
                port: 9618
                targetPort: 9618
            type: ClusterIP
      when: "'central_manager' in group_names"
      delegate_to: central_manager

    - name: Создание Service для Execute
      kubernetes.core.k8s:
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: htcondor-execute
            namespace: default
            labels:
              app: htcondor
              role: execute
          spec:
            selector:
              app: htcondor
              role: execute
            ports:
              - protocol: TCP
                port: 9618
                targetPort: 9618
                name: condor
            type: ClusterIP
      when: "'execute_nodes' in group_names"
      delegate_to: central_manager

    - name: Создание Deployment для Central Manager
      kubernetes.core.k8s:
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: htcondor-manager
            namespace: default
            labels:
              app: htcondor
              role: manager
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: htcondor
                role: manager
            template:
              metadata:
                labels:
                  app: htcondor
                  role: manager
              spec:
                nodeSelector:
                  node-role.kubernetes.io/control-plane: ""
                tolerations:
                  - key: "node-role.kubernetes.io/control-plane"
                    operator: "Exists"
                    effect: "NoSchedule"
                serviceName:
                tolerations:
                  - key: "node-role.kubernetes.io/control-plane"
                    operator: "Exists"
                    effect: "NoSchedule"
                containers:
                  - name: htcondor
                    image: "{{ condor_image }}"
                    env:
                      - name: CONDOR_CONFIG
                        value: "/etc/condor/condor_config"
                      - name: CONDOR_HOST
                        value: "htcondor-manager.default.svc.cluster.local"
                    ports:
                      - containerPort: 9618
                    volumeMounts:
                      - name: condor-config
                        mountPath: /etc/condor
                      - name: condor-passwords
                        mountPath: /etc/condor/passwords.d
                      - name: condor-logs
                        mountPath: /var/log/condor
                    command: ["sh", "-c"]
                    args:
                      - |
                        set -x
                        mkdir -p /var/log/condor
                        mkdir -p /var/lock/condor
                        chown -R usermaster:usermaster /var/log/condor
                        chown -R condor:condor /var/lock/condor /var/log/condor
                        chmod -R 777 /var/log/condor
                        chmod 775 /var/lock/condor
                        mkdir -p /etc/condor/passwords.d
                        condor_store_cred -c add -f /etc/condor/passwords.d/POOL -p "{{ condor_cluster_password }}"
                        condor_master -f
                    securityContext:
                      runAsUser: 0
                      fsGroup: 0
                volumes:
                  - name: condor-config
                    configMap:
                      name: htcondor-central
                  - name: condor-passwords
                    emptyDir: {}
                  - name: condor-logs
                    hostPath:
                      path: "{{ condor_logs_dir }}"
      when: "'central_manager' in group_names"
      delegate_to: central_manager
      run_once: true

    - name: Создание DaemonSet для Execute Nodes
      kubernetes.core.k8s:
        definition:
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: htcondor-execute
            namespace: default
          spec:
            selector:
              matchLabels:
                app: htcondor
                role: execute
            template:
              metadata:
                labels:
                  app: htcondor
                  role: execute
              spec:
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: node-role.kubernetes.io/control-plane
                              operator: DoesNotExist
                hostname: htcondor-execute-node
                tolerations:
                  - key: "node-role.kubernetes.io/control-plane"
                    operator: "Exists"
                    effect: "NoSchedule"
                  - key: "node.kubernetes.io/not-ready"
                    operator: "Exists"
                    effect: "NoExecute"
                  - key: "node.kubernetes.io/unreachable"
                    operator: "Exists"
                    effect: "NoExecute"
                initContainers:
                  - name: init-condor-locks
                    image: busybox
                    command: ['sh', '-c', 'mkdir -p /var/log/condor/locks && chown 1000:1000 /var/log/condor/locks']
                    volumeMounts:
                      - name: condor-logs
                        mountPath: /var/log/condor
                  - name: init-condor-user
                    image: busybox
                    command: ['sh', '-c', 'echo "condor:x:1000:1000::/home/condor:/bin/bash" >> /etc/passwd && echo "condor:x:1000:" >> /etc/group']
                    securityContext:
                      runAsUser: 0
                  - name: init-condor-logs
                    image: busybox
                    command: ['sh', '-c', 'mkdir -p /var/log/condor && chown -R 1000:1000 /var/log/condor']
                    volumeMounts:
                      - name: condor-logs
                        mountPath: /var/log/condor
                  - name: init-condor-password
                    image: busybox
                    command:
                      - sh
                      - -c
                      - |
                        set -x
                        mkdir -p /etc/condor/passwords.d
                        echo "{{ condor_cluster_password }}" > /etc/condor/passwords.d/POOL
                        chmod 600 /etc/condor/passwords.d/POOL
                        chown -R 1000:1000 /etc/condor/passwords.d
                    volumeMounts:
                      - name: condor-passwords
                        mountPath: /etc/condor/passwords.d
                containers:
                  - name: htcondor
                    image: "{{ condor_image }}"
                    env:
                      - name: CONDOR_CONFIG
                        value: "/etc/condor/condor_config"
                      - name: CONDOR_HOST
                        value: "htcondor-manager.default.svc.cluster.local"
                    ports:
                      - containerPort: 9618
                    volumeMounts:
                      - name: condor-config
                        mountPath: /etc/condor
                      - name: condor-logs
                        mountPath: /var/log/condor
                      - name: condor-passwords
                        mountPath: /etc/condor/passwords.d
                    command: ["/bin/bash", "-c"]
                    args:
                      - |
                        while ! nslookup htcondor-manager.default.svc.cluster.local; do
                          echo "Waiting for CONDOR_HOST...";
                          sleep 5;
                        done;

                        exec condor_master -f
                    securityContext:
                      runAsUser: 0
                      fsGroup: 0
                volumes:
                  - name: condor-config
                    configMap:
                      name: htcondor-execute
                  - name: condor-logs
                    hostPath:
                      path: "{{ condor_logs_dir }}"
                      type: DirectoryOrCreate
                  - name: condor-passwords
                    emptyDir: {}
        wait: yes
        wait_timeout: 600
      when: "'central_manager' in group_names"
      delegate_to: central_manager
      run_once: true


    - name: Создание Job для установки пароля HTCondor
      kubernetes.core.k8s:
        definition:
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: condor-password-setup
            namespace: default
          spec:
            template:
              spec:
                restartPolicy: Never
                containers:
                  - name: condor-password
                    image: "{{ condor_image }}"
                    env:
                      - name: CONDOR_CONFIG
                        value: "/etc/condor/condor_config"
                    volumeMounts:
                      - name: condor-config
                        mountPath: /etc/condor
                      - name: condor-passwords
                        mountPath: /etc/condor/passwords.d
                    command: ["sh", "-c"]
                    args:
                      - |
                        set -x
                        mkdir -p /var/log/condor
                        chmod -R 777 /var/log/condor
                        mkdir -p /etc/condor/passwords.d
                        condor_store_cred -c add -f /etc/condor/passwords.d/POOL -p "{{ condor_cluster_password }}" > /dev/null 2>&1
                        condor_master -f
                    securityContext:
                      runAsUser: 0
                      fsGroup: 0
                volumes:
                  - name: condor-config
                    configMap:
                      name: htcondor-config
                  - name: condor-passwords
                    emptyDir: {}
            backoffLimit: 1
        wait: yes
        wait_timeout: 300
      when: "'central_manager' in group_names"
